---
title: "OpenCode Serverã§å®Ÿç¾ã™ã‚‹Rails LLMçµ±åˆ - ç‰¹å®šãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ä¾å­˜ã—ãªã„AIæ´»ç”¨è¡“"
emoji: "ğŸ¤–"
type: "tech"
topics: ["rails", "ruby", "opencode", "llm", "ai"]
published: false
---

## ã¯ã˜ã‚ã«

Rubyã‚„Railsã§LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’æ´»ç”¨ã—ãŸã„ã€‚ã§ã‚‚ã€Pythonã‚„TypeScriptã«æ¯”ã¹ã‚‹ã¨AIé–¢é€£ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå°‘ãªã„...ãã‚“ãªæ‚©ã¿ã‚’æŒã£ã¦ã„ã‚‹æ–¹ã‚‚å¤šã„ã®ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹ã€‚

ä»Šå›ç´¹ä»‹ã™ã‚‹**OpenCode Server**ã‚’ä½¿ãˆã°ã€ç‰¹å®šã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ä¾å­˜ã›ãšã«LLMã‚’æ´»ç”¨ã§ãã¾ã™ã€‚ã—ã‹ã‚‚ã€Grokãªã‚‰ç„¡æ–™ã§å§‹ã‚ã‚‰ã‚Œã¾ã™ï¼

## OpenCode Serverã¨ã¯

[OpenCode](https://opencode.ai/docs/server/)ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®AIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚ãã®ä¸­ã®**Serveræ©Ÿèƒ½**ã‚’ä½¿ã†ã¨ã€HTTP APIçµŒç”±ã§LLMã¨ã‚„ã‚Šå–ã‚Šã§ãã¾ã™ã€‚

### ç‰¹å¾´

- **75ä»¥ä¸Šã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¯¾å¿œ**ï¼šOpenAIã€Anthropicã€Geminiã€Grokãªã©
- **OpenAIäº’æ›API**ï¼šæ—¢å­˜ã®OpenAIäº’æ›ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒãã®ã¾ã¾ä½¿ãˆã‚‹
- **OpenAPIä»•æ§˜**ï¼š`/doc`ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§Swagger UIã‹ã‚‰ç¢ºèªå¯èƒ½
- **è¨€èªéä¾å­˜**ï¼šHTTPçµŒç”±ãªã®ã§ã©ã®è¨€èªã‹ã‚‰ã§ã‚‚åˆ©ç”¨å¯èƒ½

## ãªãœRailsã§OpenCodeã‚’ä½¿ã†ã®ã‹

### Rubyã®ç¾çŠ¶

æ­£ç›´ãªã¨ã“ã‚ã€Rubyã¯Pythonã‚„TypeScriptã«æ¯”ã¹ã¦AI/MLé–¢é€£ã®ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ãŒå¼±ã„ã§ã™ï¼š

- **Python**ï¼šLangChainã€LlamaIndexã€OpenAI SDKãªã©å……å®Ÿ
- **TypeScript**ï¼šVercel AI SDKã€LangChain.jsãªã©æ´»ç™º
- **Ruby**ï¼š...å…¬å¼SDKãŒå°‘ãªã„ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ›´æ–°ãŒé…ã„ã“ã¨ã‚‚

### OpenCodeã§è§£æ±ºã§ãã‚‹ã“ã¨

OpenCode Serverã‚’ä¸­é–“å±¤ã¨ã—ã¦ä½¿ã†ã“ã¨ã§ï¼š

1. **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¾å­˜ã‹ã‚‰ã®è§£æ”¾** - HTTPé€šä¿¡ã ã‘ã§LLMãŒä½¿ãˆã‚‹
2. **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ‡ã‚Šæ›¿ãˆãŒå®¹æ˜“** - è¨­å®šå¤‰æ›´ã ã‘ã§åˆ¥ã®LLMã«ä¹—ã‚Šæ›ãˆå¯èƒ½
3. **Rubyã®å¼·ã¿ã‚’æ´»ã‹ã›ã‚‹** - Webé–‹ç™ºã¯Railsã§ã€AIéƒ¨åˆ†ã¯OpenCodeã«ä»»ã›ã‚‹

```
[Rails App] --HTTP--> [OpenCode Server] ---> [LLM Provider]
                                              (OpenAI, Grok, etc.)
```

## ç’°å¢ƒæ§‹ç¯‰

### 1. OpenCodeã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Homebrew (macOS/Linux)
brew install opencode

# npm
npm install -g opencode
```

### 2. OpenCode Serverã®èµ·å‹•

```bash
# ã‚µãƒ¼ãƒãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•
opencode serve --port 4096

# ã¾ãŸã¯ TUI ã¨ä¸€ç·’ã«èµ·å‹•ã—ã¦ãƒãƒ¼ãƒˆæŒ‡å®š
opencode --port 4096
```

èµ·å‹•å¾Œã€`http://localhost:4096/doc` ã§OpenAPIä»•æ§˜ã‚’ç¢ºèªã§ãã¾ã™ã€‚

### 3. Grokã®è¨­å®šï¼ˆç„¡æ–™ã§ä½¿ãˆã‚‹ï¼ï¼‰

Grokã¯[xAI Console](https://console.x.ai/)ã§APIã‚­ãƒ¼ã‚’å–å¾—ã§ãã¾ã™ã€‚

**ãƒ‡ãƒ¼ã‚¿å…±æœ‰ã«ã‚ªãƒ—ãƒˆã‚¤ãƒ³ã™ã‚‹ã¨ã€æ¯æœˆ$150åˆ†ã®ç„¡æ–™ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆãŒã‚‚ã‚‰ãˆã¾ã™ï¼**

:::message
æ³¨æ„ï¼šãƒ‡ãƒ¼ã‚¿å…±æœ‰ã«åŒæ„ã™ã‚‹ã¨ã€ã‚„ã‚Šå–ã‚Šã®å†…å®¹ãŒxAIã®åˆ†æã«ä½¿ã‚ã‚Œã¾ã™ã€‚å•†ç”¨åˆ©ç”¨ã®å ´åˆã¯åˆ¥ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚
:::

OpenCodeã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`~/.opencode/config.json`ï¼‰ã«è¿½åŠ ï¼š

```json
{
  "providers": {
    "xai": {
      "apiKey": "your-xai-api-key"
    }
  },
  "model": "xai/grok-3"
}
```

## Railsã§ã®å®Ÿè£…

### åŸºæœ¬çš„ãªLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

```ruby
# app/services/llm_client.rb
class LlmClient
  OPENCODE_URL = ENV.fetch('OPENCODE_URL', 'http://localhost:4096')

  def initialize
    @conn = Faraday.new(url: OPENCODE_URL) do |f|
      f.request :json
      f.response :json
      f.adapter Faraday.default_adapter
    end
  end

  def chat(messages:, model: nil)
    response = @conn.post('/chat/completions') do |req|
      req.body = {
        messages: messages,
        model: model
      }.compact
    end

    response.body
  end

  def simple_ask(question)
    chat(messages: [{ role: 'user', content: question }])
  end
end
```

### Controllerã§ã®åˆ©ç”¨

```ruby
# app/controllers/ai_controller.rb
class AiController < ApplicationController
  def ask
    client = LlmClient.new
    result = client.simple_ask(params[:question])

    render json: {
      answer: result.dig('choices', 0, 'message', 'content')
    }
  end
end
```

### Active Jobã§ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†

LLMã®å¿œç­”ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ã€Active Jobã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†ï¼š

```ruby
# app/jobs/llm_process_job.rb
class LlmProcessJob < ApplicationJob
  queue_as :default

  def perform(prompt:, callback_url:)
    client = LlmClient.new
    result = client.simple_ask(prompt)

    # çµæœã‚’ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ or DBã«ä¿å­˜
    LlmResult.create!(
      prompt: prompt,
      response: result.dig('choices', 0, 'message', 'content')
    )
  end
end
```

### ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ

ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å¿œç­”ã‚’è¡¨ç¤ºã—ãŸã„å ´åˆï¼š

```ruby
# app/services/llm_streaming_client.rb
class LlmStreamingClient
  include ActionController::Live

  def stream_chat(messages:, &block)
    uri = URI("#{OPENCODE_URL}/chat/completions")

    Net::HTTP.start(uri.host, uri.port) do |http|
      request = Net::HTTP::Post.new(uri)
      request['Content-Type'] = 'application/json'
      request.body = {
        messages: messages,
        stream: true
      }.to_json

      http.request(request) do |response|
        response.read_body do |chunk|
          yield chunk if block_given?
        end
      end
    end
  end
end
```

## å®Ÿè·µçš„ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹

### 1. å•†å“èª¬æ˜ã®è‡ªå‹•ç”Ÿæˆ

```ruby
# app/services/product_description_generator.rb
class ProductDescriptionGenerator
  def generate(product)
    client = LlmClient.new

    prompt = <<~PROMPT
      ä»¥ä¸‹ã®å•†å“æƒ…å ±ã‚’ã‚‚ã¨ã«ã€é­…åŠ›çš„ãªå•†å“èª¬æ˜ã‚’200æ–‡å­—ç¨‹åº¦ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

      å•†å“å: #{product.name}
      ã‚«ãƒ†ã‚´ãƒª: #{product.category}
      ç‰¹å¾´: #{product.features.join(', ')}
    PROMPT

    result = client.simple_ask(prompt)
    result.dig('choices', 0, 'message', 'content')
  end
end
```

### 2. ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®è‡ªå‹•å¿œç­”

```ruby
# app/services/support_assistant.rb
class SupportAssistant
  SYSTEM_PROMPT = <<~PROMPT
    ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆæ‹…å½“ã§ã™ã€‚
    ä»¥ä¸‹ã®FAQã‚’å‚è€ƒã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
    ã‚ã‹ã‚‰ãªã„å ´åˆã¯ã€Œæ‹…å½“è€…ã«ãŠç¹‹ãã—ã¾ã™ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
  PROMPT

  def respond(user_message, faq_context)
    client = LlmClient.new

    client.chat(messages: [
      { role: 'system', content: "#{SYSTEM_PROMPT}\n\nFAQ:\n#{faq_context}" },
      { role: 'user', content: user_message }
    ])
  end
end
```

### 3. ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ

```ruby
# app/services/code_reviewer.rb
class CodeReviewer
  def review(code, language: 'ruby')
    client = LlmClient.new

    prompt = <<~PROMPT
      ä»¥ä¸‹ã®#{language}ã‚³ãƒ¼ãƒ‰ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ãã ã•ã„ã€‚
      æ”¹å–„ç‚¹ãŒã‚ã‚Œã°å…·ä½“çš„ã«æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚

      ```#{language}
      #{code}
      ```
    PROMPT

    client.simple_ask(prompt)
  end
end
```

## ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ‡ã‚Šæ›¿ãˆ

OpenCodeã®æœ€å¤§ã®åˆ©ç‚¹ã¯ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã™ã‚‹ã ã‘ã§LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹ã“ã¨ã§ã™ã€‚

```json
// Grok ã‚’ä½¿ã†å ´åˆ
{ "model": "xai/grok-3" }

// OpenAI ã‚’ä½¿ã†å ´åˆ
{ "model": "openai/gpt-4" }

// Anthropic ã‚’ä½¿ã†å ´åˆ
{ "model": "anthropic/claude-3-5-sonnet" }

// ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆLM StudioçµŒç”±ï¼‰ã‚’ä½¿ã†å ´åˆ
{
  "providers": {
    "lmstudio": {
      "baseURL": "http://localhost:1234/v1"
    }
  },
  "model": "lmstudio/local-model"
}
```

**Railsã‚¢ãƒ—ãƒªã®ã‚³ãƒ¼ãƒ‰ã¯ä¸€åˆ‡å¤‰æ›´ä¸è¦ï¼**

## ãƒ†ã‚¹ãƒˆ

```ruby
# spec/services/llm_client_spec.rb
RSpec.describe LlmClient do
  describe '#simple_ask' do
    it 'returns a response from the LLM' do
      # OpenCodeã®ãƒ¢ãƒƒã‚¯ã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ã†ã‹ã€WebMockã§ã‚¹ã‚¿ãƒ–
      stub_request(:post, "http://localhost:4096/chat/completions")
        .to_return(
          status: 200,
          body: {
            choices: [{ message: { content: 'Hello!' } }]
          }.to_json
        )

      client = LlmClient.new
      result = client.simple_ask('Say hello')

      expect(result.dig('choices', 0, 'message', 'content')).to eq('Hello!')
    end
  end
end
```

## ã¾ã¨ã‚

OpenCode Serverã‚’ä½¿ã†ã“ã¨ã§ï¼š

- **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ä¾å­˜ã—ãªã„** - HTTPé€šä¿¡ã ã‘ã§LLMãŒä½¿ãˆã‚‹
- **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è‡ªç”±ã«é¸ã¹ã‚‹** - Grokï¼ˆç„¡æ–™ï¼‰ã‹ã‚‰OpenAIã€Anthropicã¾ã§
- **Railsã®å¼·ã¿ã‚’æ´»ã‹ã›ã‚‹** - Webé–‹ç™ºã¯Railsã§ã€AIéƒ¨åˆ†ã¯OpenCodeã«å§”è­²

Rubyã®AIã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ãŒå¼±ã„ã‹ã‚‰ã¨ã„ã£ã¦è«¦ã‚ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚OpenCodeã¨ã„ã†ã€Œç¿»è¨³å±¤ã€ã‚’æŒŸã‚€ã“ã¨ã§ã€ã©ã®è¨€èªã‹ã‚‰ã§ã‚‚LLMã‚’æ´»ç”¨ã§ãã¾ã™ã€‚

ç‰¹ã«Grokã®ç„¡æ–™ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆï¼ˆæœˆ$150ï¼‰ã‚’æ´»ç”¨ã™ã‚Œã°ã€ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆãªãŒã‚‰AIæ©Ÿèƒ½ã‚’è©¦ã›ã¾ã™ã€‚ãœã²è©¦ã—ã¦ã¿ã¦ãã ã•ã„ï¼

## å‚è€ƒãƒªãƒ³ã‚¯

- [OpenCode Server ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://opencode.ai/docs/server/)
- [OpenCode SDK](https://opencode.ai/docs/sdk/)
- [OpenCode ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š](https://opencode.ai/docs/providers/)
- [xAI Consoleï¼ˆGrok APIï¼‰](https://console.x.ai/)
- [Grok API ç„¡æ–™ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã®ä½¿ã„æ–¹](https://zenn.dev/sunwood_ai_labs/articles/trying-grok-api-free-credit)
